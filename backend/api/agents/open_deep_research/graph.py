########## graph.py (UPDATED CODE) ##########
import functools
import json
import time
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor
from typing import Literal, List, Optional, Tuple, Any, Callable
import os
import re
import weave
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
from langchain_sambanova import ChatSambaNovaCloud
from langchain_fireworks import ChatFireworks
from langchain_core.callbacks import BaseCallbackHandler

from langgraph.constants import Send
from langgraph.graph import START, END, StateGraph
from langgraph.types import interrupt, Command
from api.services.redis_service import SecureRedisService

from config.model_registry import model_registry

import weave

# We import our data models from the api module

from .state import (
    ReportStateInput,
    ReportStateOutput,
    Sections,
    ReportState,
    SectionState,
    SectionOutputState,
    Queries,
    Feedback,
)
from .prompts import (
    report_planner_query_writer_instructions,
    report_planner_instructions,
    query_writer_instructions,
    section_writer_instructions,
    final_section_writer_instructions,
    section_grader_instructions,
)
from .configuration import Configuration, SearchAPI
from .utils import tavily_search_async, deduplicate_and_format_sources, format_sections, perplexity_search

# We import our data models
from api.data_types import (
    DeepResearchReport,
    DeepResearchSection,
    DeepCitation
)

from utils.logging import logger

class UsageCallback(BaseCallbackHandler):
    def __init__(self, provider: str):
        self.usage = []
        self.provider = provider
        
    def on_llm_end(self, response, **kwargs):
        if self.provider == "sambanova":
            if hasattr(response, 'generations'):
                for generation_list in response.generations:
                    for generation in generation_list:
                        if hasattr(generation.message, 'response_metadata'):
                            metadata_usage = generation.message.response_metadata.get('usage', {})
                            if metadata_usage:
                                self.usage.append(metadata_usage)
        elif self.provider == "fireworks":
            if hasattr(response, 'generations'):
                for generation_list in response.generations:
                    for generation in generation_list:
                        if hasattr(generation.message, 'response_metadata'):
                            metadata_usage = generation.message.response_metadata.get('token_usage', {})
                            if metadata_usage:
                                self.usage.append(metadata_usage)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

class LLMTimeoutError(Exception):
    """Custom exception for LLM timeout."""
    def __init__(self, message="LLM request timed out after the specified duration"):
        self.message = message
        super().__init__(self.message)

def get_model_name(llm):
    if hasattr(llm, 'model_name'):
        return llm.model_name
    elif hasattr(llm, 'model'):
        return llm.model
    else:
        return "Unknown Model"

def create_publish_callback(
    user_id: str,
    conversation_id: str,
    message_id: str,
    agent_name: str,
    workflow_name: str,
    redis_client: SecureRedisService,
    token_usage_callback: Callable[[dict], None] = None,
):
   
    def callback(message: str, llm_name: str, task: str, usage: dict, llm_provider: str, duration: float):
        response_duration = usage.get("end_time", 0) - usage.get("start_time", 0)
        if response_duration > 0:
            duration = response_duration

        # Track token usage if callback provided
        if token_usage_callback and usage:
            token_usage = {
                "total_tokens": usage.get("total_tokens", 0),
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
            }
            token_usage_callback(token_usage)

        message_data = {
            "user_id": user_id,
            "run_id": conversation_id,
            "message_id": message_id,
            "agent_name": agent_name,
            "text": message,
            "timestamp": time.time(),
            "metadata": {
                "workflow_name": workflow_name,
                "agent_name": agent_name,
                "llm_name": llm_name,
                "llm_provider": llm_provider,
                "task": task,
                "total_tokens": usage.get("total_tokens", 0),
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "acceptance_rate": usage.get("acceptance_rate", 0),
                "completion_tokens_after_first_per_sec": usage.get("completion_tokens_after_first_per_sec", 0),
                "completion_tokens_after_first_per_sec_first_ten": usage.get("completion_tokens_after_first_per_sec_first_ten", 0),
                "completion_tokens_per_sec": usage.get("completion_tokens_per_sec", 0),
                "time_to_first_token": usage.get("time_to_first_token", 0),
                "total_latency": usage.get("total_latency", 0),
                "total_tokens_per_sec": usage.get("total_tokens_per_sec", 0),
                "duration": duration
            },
        }
        channel = f"agent_thoughts:{user_id}:{conversation_id}"
        redis_client.publish(channel, json.dumps(message_data))
    
    return callback


###############################################################################
# 1) Utility: parse a reference line from the sources block
###############################################################################
def parse_reference_line(line: str) -> DeepCitation:
    """
    If the line has an http/https link, store that as url, everything before is "title".
    If no URL found, return a citation with empty url => skip it upstream.
    """
    # remove bullet chars
    line = line.lstrip("*-0123456789. ").strip()
    # find a link
    match = re.search(r'(https?://[^\s]+)', line)
    if not match:
        # no link => skip
        return DeepCitation(title="", url="")  # no URL
    url = match.group(1)
    # everything prior to the url is the title
    idx = line.find(url)
    title = line[:idx].strip(" :")
    return DeepCitation(title=title, url=url)

def extract_sources_block(section_text: str) -> Tuple[str, List[DeepCitation]]:
    """
    If there's a block after ### Sources or ## Sources or 'Sources:' 
    parse them line by line. 
    Remove that block from the text. Return cleaned text + references array.
    """
    lines = section_text.split("\n")
    cleaned_lines = []
    citations: List[DeepCitation] = []

    in_sources = False

    i = 0
    while i < len(lines):
        line = lines[i].rstrip()
        lower_line = line.lower().strip()
        # detect the start
        if not in_sources and (lower_line == "### sources" or lower_line == "## sources" or lower_line == "sources:"):
            # from next line onward => references
            in_sources = True
            i += 1
            continue

        if in_sources:
            # if blank or new heading => end
            if not line.strip() or re.match(r'#+\s', line):
                # end
                in_sources = False
                # do not add line to cleaned
                i += 1
                continue
            # parse the line
            ref = parse_reference_line(line)
            # if no url => skip
            if ref.url:
                citations.append(ref)
        else:
            cleaned_lines.append(line)
        i += 1

    new_text = "\n".join(cleaned_lines).strip()
    return new_text, citations

###############################################################################
# 2) Optional remove inline references with a pattern
###############################################################################
def remove_inline_citation_lines(text: str) -> Tuple[str, List[DeepCitation]]:
    """
    If you want to forcibly remove lines that look like a bullet with 'http' 
    but are not in the sources block, you can parse them here.
    We'll only remove lines that start with "* " or "- " or some bullet + a link.
    """
    lines = text.split("\n")
    new_lines = []
    found: List[DeepCitation] = []

    for line in lines:
        trimmed = line.strip()
        # check if it starts with bullet
        if re.match(r'^(\*|-)\s+', trimmed):
            # check for a url
            match = re.search(r'(https?://[^\s]+)', trimmed)
            if match:
                # parse
                ref = parse_reference_line(trimmed)
                if ref.url:  # store
                    found.append(ref)
                # skip line
                continue
        # else keep
        new_lines.append(line)
    return "\n".join(new_lines), found

###############################################################################
# The normal nodes
###############################################################################

def get_session_id_from_config(config: Configuration) -> Optional[str]:
    """
    Extract and format session ID from RunnableConfig.
    Returns None if user_id or conversation_id is missing.
    """
    if config.user_id and config.conversation_id:
        return f"{config.user_id}:{config.conversation_id}"
    return None

async def generate_report_plan(writer_model, planner_model, state: ReportState, config: RunnableConfig):
    topic = state["topic"]
    feedback = state.get("feedback_on_report_plan", None)

    configurable = Configuration.from_runnable_config(config)
    usage_handler_report_planner = UsageCallback(provider=configurable.provider)
    usage_handler_query_generation = UsageCallback(provider=configurable.provider)

    report_structure = configurable.report_structure
    number_of_queries = configurable.number_of_queries 
    session_id = get_session_id_from_config(configurable)

    logger.info(logger.format_message(session_id, f"Generating report plan for topic: {topic}"))
    if feedback:
        logger.info(logger.format_message(session_id, f"Incorporating feedback: {feedback}"))

    if isinstance(report_structure, dict):
        report_structure = str(report_structure)

    structured_llm = writer_model.with_structured_output(Queries)
    
    system_instructions_query = report_planner_query_writer_instructions.format(
        topic=topic,
        report_organization=report_structure,
        number_of_queries=number_of_queries
    )
    
    llm_config_query = RunnableConfig(callbacks=[usage_handler_query_generation], tags=["query_generation"])
    logger.info(logger.format_message(session_id, "Generating initial search queries for planning"))
    
    results = invoke_llm_with_tracking(
        llm=structured_llm,
        messages=[
            SystemMessage(content=system_instructions_query),
            HumanMessage(content="Generate search queries that will help with planning the sections of the report.")
        ],
        task="Generate initial planning queries",
        config=llm_config_query,
        usage_handler=usage_handler_query_generation,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(writer_model)
    )

    query_list = [q.search_query for q in results.queries]
    logger.info(logger.format_message(session_id, f"Generated {len(query_list)} search queries"))

    logger.info(logger.format_message(session_id, f"Using search API: {configurable.search_api}"))
    if configurable.search_api == SearchAPI.TAVILY:
        logger.info(logger.format_message(session_id, f"Using Tavily API with key rotation for {len(query_list)} queries"))
        search_results = await tavily_search_async(query_list, configurable.api_key_rotator)
        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1500, include_raw_content=False)
    elif configurable.search_api == SearchAPI.PERPLEXITY:
        search_results = perplexity_search(query_list, configurable.api_key_rotator)
        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)
    else:
        logger.error(logger.format_message(session_id, f"Unsupported search API: {configurable.search_api}"))
        raise ValueError(f"Unsupported search API: {configurable.search_api}")

    logger.info(logger.format_message(session_id, "Generating final report sections plan"))
    system_instructions_sections = report_planner_instructions.format(
        topic=topic,
        report_organization=report_structure,
        context=source_str,
        feedback=feedback
    )

    structured_llm = planner_model.with_structured_output(Sections)
    llm_config_sections = RunnableConfig(callbacks=[usage_handler_report_planner], tags=["report_planning"])
    
    report_sections = invoke_llm_with_tracking(
        llm=structured_llm,
        messages=[
            SystemMessage(content=system_instructions_sections),
            HumanMessage(content="Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. Each section must have: name, description, plan, research, and content fields.")
        ],
        task="Generate report sections plan",
        config=llm_config_sections,
        usage_handler=usage_handler_report_planner,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(planner_model)
    )

    sections = report_sections.sections
    logger.info(logger.format_message(session_id, f"Generated plan with {len(sections)} sections"))
    return {"sections": sections}

def human_feedback(state: ReportState, config: RunnableConfig) -> Command[Literal["generate_report_plan","build_section_with_web_research", "summarize_documents"]]:
    sections = state["sections"]
    sec_str = "\n\n".join(
        f"<b>Section {i+1}:</b> {s.name} - {s.description}\n"
        for i, s in enumerate(sections)
    )
    fb = interrupt(sec_str)
    if isinstance(fb, bool):
        if state.get("document"):
            return Command(goto="summarize_documents")
        else:
            return Command(goto=[
                Send("build_section_with_web_research", {"section": s, "search_iterations": 0})
                for s in sections
                if s.research
            ])
    elif isinstance(fb, str):
        return Command(goto="generate_report_plan", update={"feedback_on_report_plan": fb})
    else:
        raise ValueError("interrupt unknown")

def generate_queries(writer_model, state: SectionState, config: RunnableConfig):
    sec = state["section"]
    configurable = Configuration.from_runnable_config(config)
    usage_handler = UsageCallback(provider=configurable.provider)
    session_id = get_session_id_from_config(configurable)

    logger.info(logger.format_message(session_id, f"Generating queries for section: {sec.name}"))

    structured_llm = writer_model.with_structured_output(Queries)
    sys_inst = query_writer_instructions.format(
        section_topic=sec.description,
        number_of_queries=configurable.number_of_queries
    )
    llm_config = RunnableConfig(
        callbacks=[usage_handler],
        tags=["query_generation"]
    )
    
    queries = invoke_llm_with_tracking(
        llm=structured_llm,
        messages=[
            SystemMessage(content=sys_inst),
            HumanMessage(content="Generate search queries.")
        ],
        task="Generate search queries",
        config=llm_config,
        usage_handler=usage_handler,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(writer_model)
    )
    
    logger.info(
        logger.format_message(
            session_id,
            f"Generated {len(queries.queries)} queries for section {sec.name}."
        )
    )
    return {"search_queries": queries.queries}

async def search_web(state: SectionState, config: RunnableConfig):
    sq = state["search_queries"]
    configurable = Configuration.from_runnable_config(config)
    session_id = get_session_id_from_config(configurable)
    
    query_list = [q.search_query for q in sq]
    logger.info(logger.format_message(session_id, f"Executing web search with {len(query_list)} queries"))

    logger.info(logger.format_message(session_id, f"Using search API: {configurable.search_api}"))
    if configurable.search_api == SearchAPI.TAVILY:
        logger.info(logger.format_message(session_id, f"Using Tavily API with key rotation for {len(query_list)} queries"))
        search_results = await tavily_search_async(query_list, configurable.api_key_rotator)
        src_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1500, include_raw_content=True)
    elif configurable.search_api == SearchAPI.PERPLEXITY:
        search_results = perplexity_search(query_list, configurable.api_key_rotator)
        src_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=5000, include_raw_content=False)
    else:
        logger.error(logger.format_message(session_id, f"Unsupported search API: {configurable.search_api}"))
        raise ValueError(f"Unsupported search API: {configurable.search_api}")

    logger.info(logger.format_message(session_id, "Successfully completed web search"))
    return {
        "source_str": src_str,
        "search_iterations": state["search_iterations"] + 1
    }

def invoke_llm_with_tracking(
    llm,
    messages: List[Any],
    task: str,
    llm_name: str,
    config: RunnableConfig,
    usage_handler: UsageCallback,
    configurable: Configuration,
    session_id: Optional[str] = None,
    timeout_seconds: int = 120 
) -> Any:
    """Helper function to invoke LLM with timing and usage tracking.
    
    Args:
        llm: The LLM to invoke
        messages: List of messages to send to the LLM
        task: Description of the task being performed
        config: RunnableConfig for the LLM
        usage_handler: UsageCallback instance to track usage
        configurable: Configuration instance
        session_id: Optional session ID for logging
        timeout_seconds: Maximum time in seconds to wait for LLM response
    
    Returns:
        The LLM response
        
    Raises:
        LLMTimeoutError: If the LLM request exceeds the timeout duration
    """
    start_time = time.time()
    
    # Create an event to signal completion
    completion_event = threading.Event()
    response_container = []
    exception_container = []
    
    # Function to run in a separate thread
    def invoke_llm_thread():
        try:
            result = llm.invoke(messages, config=config)
            response_container.append(result)
        except Exception as e:
            exception_container.append(e)
        finally:
            completion_event.set()
    
    # Start the thread
    thread = threading.Thread(target=invoke_llm_thread)
    thread.daemon = True  # Allow the thread to be terminated when the main thread exits
    thread.start()
    
    # Wait for completion or timeout
    if not completion_event.wait(timeout=timeout_seconds):
        error_msg = f"LLM {llm_name} request timed out after {timeout_seconds} seconds for task {task}"
        logger.error(logger.format_message(session_id, error_msg))
        # Thread will continue running but we'll ignore its result
        raise LLMTimeoutError(error_msg)
    
    # Check if there was an exception
    if exception_container:
        raise exception_container[0]
    
    # Get the response
    if not response_container:
        raise RuntimeError(f"No response received from LLM {llm_name} for task {task}")
    
    response = response_container[0]
    duration = time.time() - start_time
    
    if duration > 10:
        logger.warning(logger.format_message(session_id, f"Deep Research - LLM {llm_name} took {duration:.2f} seconds to complete task {task}"))
    else:
        logger.info(logger.format_message(session_id, f"Deep Research - LLM {llm_name} took {duration:.2f} seconds to complete task {task}"))

    if usage_handler.usage and len(usage_handler.usage) > 1:
        logger.warning(logger.format_message(session_id, f"Multiple usage objects found in callback. Using the first one."))

    if isinstance(response, AIMessage):
        text = response.content
    else:
        text = response.model_dump()
    
    # Only call the callback if we have usage data
    if usage_handler.usage:
        configurable.callback(
            message=text,
            task=task,
            llm_name=llm_name,
            llm_provider=configurable.provider,
            usage=usage_handler.usage[0],
            duration=duration
        )
    else:
        # Log that we're missing usage data but still call the callback with empty usage
        logger.warning(logger.format_message(session_id, f"No usage data available for {llm_name} on task {task}"))
        configurable.callback(
            message=text,
            task=task,
            llm_name=llm_name,
            llm_provider=configurable.provider,
            usage={},
            duration=duration
        )

    return response

def write_section(
    writer_model, state: SectionState, config: RunnableConfig
) -> Command[Literal["__end__", "search_web"]]:
    sec = state["section"]
    configurable = Configuration.from_runnable_config(config)
    usage_handler_section_writing = UsageCallback(provider=configurable.provider)
    usage_handler_section_grading = UsageCallback(provider=configurable.provider)
    session_id = get_session_id_from_config(configurable)

    logger.info(logger.format_message(session_id, f"Writing section: {sec.name}"))
    src = state["source_str"]
    doc_summary = state.get("document_summary", "")
    sys_inst = section_writer_instructions.format(
        section_title=sec.name,
        section_topic=sec.description,
        context=src,
        section_content=sec.content,
        document_summary=doc_summary
    )

    llm_config_section_writing = RunnableConfig(callbacks=[usage_handler_section_writing], tags=["section_writing"])
    
    content = invoke_llm_with_tracking(
        llm=writer_model,
        messages=[SystemMessage(content=sys_inst), HumanMessage(content="Write the section.")],
        task=f"Write section - {sec.name}",
        config=llm_config_section_writing,
        usage_handler=usage_handler_section_writing,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(writer_model)
    )
    
    sec.content = content.content
    logger.info(
        logger.format_message(session_id, f"Generated content for section: {sec.name}")
    )

    # now we grade
    logger.info(logger.format_message(session_id, f"Grading section: {sec.name}"))
    grader_inst = section_grader_instructions.format(
        section_topic=sec.description, section=sec.content
    )
    llm_config_section_grading = RunnableConfig(callbacks=[usage_handler_section_grading], tags=["section_grading"])
    structured_llm = writer_model.with_structured_output(Feedback)

    fb = invoke_llm_with_tracking(
        llm=structured_llm,
        messages=[SystemMessage(content=grader_inst), HumanMessage(content="Grade it")],
        task=f"Grade section - {sec.name}",
        config=llm_config_section_grading,
        usage_handler=usage_handler_section_grading,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(writer_model)
    )

    if fb.grade == "pass":
        logger.info(logger.format_message(session_id, f"Section {sec.name} passed grading"))
        return Command(update={"completed_sections": [sec]}, goto=END)
    elif state["search_iterations"] >= Configuration.from_runnable_config(config).max_search_depth:
        logger.warning(logger.format_message(session_id, f"Section {sec.name} reached max iterations, moving on"))
        return Command(update={"completed_sections": [sec]}, goto=END)
    else:
        logger.info(logger.format_message(session_id, f"Section {sec.name} needs revision, doing another search iteration"))
        return Command(
            update={"search_queries": fb.follow_up_queries, "section": sec},
            goto="search_web"
        )


def write_final_sections(writer_model, state: SectionState, config: RunnableConfig):
    sec = state["section"]
    configurable = Configuration.from_runnable_config(config)
    usage_handler = UsageCallback(provider=configurable.provider)
    session_id = get_session_id_from_config(configurable)

    logger.info(logger.format_message(session_id, f"Writing final section: {sec.name}"))
    rep = state["report_sections_from_research"]

    sys_inst = final_section_writer_instructions.format(
        section_title=sec.name, section_topic=sec.description, context=rep
    )
    llm_config = RunnableConfig(
        callbacks=[usage_handler], tags=["final_section_writing"]
    )

    content = invoke_llm_with_tracking(
        llm=writer_model,
        messages=[SystemMessage(content=sys_inst), HumanMessage(content="Write final section.")],
        task="Write final section",
        config=llm_config,
        usage_handler=usage_handler,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(writer_model)
    )

    sec.content = content.content
    logger.info(logger.format_message(session_id, f"Completed final section: {sec.name}"))
    return {"completed_sections": [sec]}

def gather_completed_sections(state: ReportState, config: RunnableConfig):
    comps = state["completed_sections"]
    configurable = Configuration.from_runnable_config(config)
    session_id = get_session_id_from_config(configurable)
    
    logger.info(logger.format_message(session_id, f"Gathering {len(comps)} completed sections"))
    rep = format_sections(comps)
    return {"report_sections_from_research": rep}

def initiate_final_section_writing(state: ReportState, config: RunnableConfig):
    configurable = Configuration.from_runnable_config(config)
    session_id = get_session_id_from_config(configurable)
    
    non_research_sections = [s for s in state["sections"] if not s.research]
    logger.info(logger.format_message(session_id, f"Initiating writing of {len(non_research_sections)} final sections"))
    return [
        Send("write_final_sections", {"section": s, "report_sections_from_research": state["report_sections_from_research"]})
        for s in non_research_sections
    ]

def compile_final_report(state: ReportState, config: RunnableConfig):
    configurable = Configuration.from_runnable_config(config)
    session_id = get_session_id_from_config(configurable)
    
    sections = state["sections"]
    logger.info(logger.format_message(session_id, "Compiling final report"))
    completed_map = {s.name: s.content for s in state["completed_sections"]}

    deep_sections: List[DeepResearchSection] = []
    all_citations: List[DeepCitation] = []

    for sec in sections:
        content = completed_map.get(sec.name, sec.content or "")

        # 1) strip any sources block
        cleaned, block_refs = extract_sources_block(content)
        # 2) optionally remove bullet lines that contain URLs but are not in sources block
        # if we want to keep inline links in the text, skip this step or tweak it
        final_cleaned, inline_refs = remove_inline_citation_lines(cleaned)

        # combine
        # only keep references that have a valid url
        block_refs = [r for r in block_refs if r.url.strip()]
        inline_refs = [r for r in inline_refs if r.url.strip()]
        all_citations.extend(block_refs)
        all_citations.extend(inline_refs)

        deep_sections.append(
            DeepResearchSection(
                name=sec.name,
                description=sec.description,
                content=final_cleaned.strip(),
                citations=[]  # we skip local citations array
            )
        )

    # build final text
    lines = []
    for ds in deep_sections:
        lines.append(f"# {ds.name}\n{ds.content}\n")

    # final citations at end
    if all_citations:
        lines.append("## Citations\n")
        for c in all_citations:
            title = c.title.strip() or "Untitled"
            url = c.url.strip()
            lines.append(f"- [{title}]({url})")

    final_text = "\n".join(lines).strip()

    logger.info(logger.format_message(session_id, f"Completed report compilation with {len(deep_sections)} sections and {len(all_citations)} citations"))
    report = DeepResearchReport(
        sections=deep_sections,
        final_report=final_text,
        citations=all_citations
    )
    return {"final_report": final_text, "deep_research_report": report}

def summarize_documents(summary_model, state: ReportState, config: RunnableConfig):
    """
    Summarize provided documents if they exist.
    """
    configurable = Configuration.from_runnable_config(config)
    usage_handler = UsageCallback(provider=configurable.provider)
    session_id = get_session_id_from_config(configurable)
    
    document = state.get("document", None)
    if not document:
        logger.info(logger.format_message(session_id, "No documents provided for summarization"))
        return {"document_summary": ""}
        
    logger.info(logger.format_message(session_id, f"Summarizing a {len(document)} word document"))
    
    
    sys_inst = """You are a document summarizer. Your task is to:
    1. Read through the provided documents
    2. Extract the key information, main points, and important findings
    3. Create a comprehensive but concise summary that captures the essential information
    4. Focus on factual information that would be relevant for research
    5. Maintain objectivity and accuracy
    
    Format your response as a clear, well-structured summary."""

    llm_config = RunnableConfig(callbacks=[usage_handler], tags=["document_summarization"])
    
    summary = invoke_llm_with_tracking(
        llm=summary_model,
        messages=[
            SystemMessage(content=sys_inst),
            HumanMessage(content=f"Please summarize the following document:\n\n{document}")
        ],
        task="Summarize provided document",
        config=llm_config,
        usage_handler=usage_handler,
        configurable=configurable,
        session_id=session_id,
        llm_name=get_model_name(summary_model)
    )

    # After summarization, initiate section building
    sections = state.get("sections", [])
    return Command(goto=[
                Send("build_section_with_web_research", {"section": s, "search_iterations": 0, "document_summary": summary.content})
                for s in sections
                if s.research
    ])
@weave.op
def get_graph(api_key: str, provider: str):
    """
    Create and configure the graph for deep research.
    
    Args:
        api_key: The API key for the LLM provider
        provider: The LLM provider to use (fireworks or sambanova)
        documents: Optional list of documents to process
    """
    model_name = "llama-3.3-70b"
    planner_model_config: str = model_registry.get_model_info(model_key=model_name, provider=provider)
    writer_model_config: str = model_registry.get_model_info(model_key=model_name, provider=provider)
    summary_model_config: str = model_registry.get_model_info(model_key=model_name, provider=provider)

    if provider == "fireworks":    
        writer_model = ChatFireworks(base_url=writer_model_config["url"], model=writer_model_config["model"], temperature=0, max_tokens=8192, api_key=api_key)
        planner_model = ChatFireworks(base_url=planner_model_config["url"], model=planner_model_config["model"], temperature=0, max_tokens=8192, api_key=api_key)
        summary_model = ChatFireworks(base_url=summary_model_config["url"], model=summary_model_config["model"], temperature=0, max_tokens=8192, api_key=api_key)
    elif provider == "sambanova":
        writer_model = ChatSambaNovaCloud(sambanova_url=writer_model_config["long_url"], model=writer_model_config["model"], temperature=0, max_tokens=8192, sambanova_api_key=api_key)
        planner_model = ChatSambaNovaCloud(sambanova_url=planner_model_config["long_url"], model=planner_model_config["model"], temperature=0, max_tokens=8192, sambanova_api_key=api_key)
        summary_model = ChatSambaNovaCloud(sambanova_url=summary_model_config["long_url"], model=summary_model_config["model"], temperature=0, max_tokens=8192, sambanova_api_key=api_key)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

    section_builder = StateGraph(SectionState, output=SectionOutputState)
    section_builder.add_node("generate_queries", functools.partial(generate_queries, writer_model))
    section_builder.add_node("search_web", search_web)
    section_builder.add_node("write_section", functools.partial(write_section, writer_model))

    section_builder.add_edge(START, "generate_queries")
    section_builder.add_edge("generate_queries", "search_web")
    section_builder.add_edge("search_web", "write_section")

    builder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=Configuration)
    builder.add_node("generate_report_plan", functools.partial(generate_report_plan, writer_model, planner_model))
    builder.add_node("human_feedback", human_feedback)
    builder.add_node("summarize_documents", functools.partial(summarize_documents, summary_model))
    builder.add_node("build_section_with_web_research", section_builder.compile())
    builder.add_node("gather_completed_sections", gather_completed_sections)
    builder.add_node("write_final_sections", functools.partial(write_final_sections, writer_model))
    builder.add_node("compile_final_report", compile_final_report)

    builder.add_edge(START, "generate_report_plan")
    builder.add_edge("generate_report_plan", "human_feedback")
    builder.add_edge("build_section_with_web_research", "gather_completed_sections")
    builder.add_conditional_edges("gather_completed_sections", initiate_final_section_writing, ["write_final_sections"])
    builder.add_edge("write_final_sections", "compile_final_report")
    builder.add_edge("compile_final_report", END)

    return builder
